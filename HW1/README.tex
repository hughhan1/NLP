\documentclass[11pt]{article}

\usepackage{enumitem}               
 
\usepackage{titlesec}

\usepackage{listings}
\usepackage{color}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language={},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}

\newcommand{\N}{\ensuremath{\mathbb N}}

\newcommand{\prob}[1]{\ensuremath{\text{{\bf Pr}$\left[#1\right]$}}}
\newcommand{\expct}[1]{\ensuremath{\text{{\bf E}$\left[#1\right]$}}}

\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\codebox}[1]{\colorbox{codegray}{\texttt{#1}}}

\newcommand{\mnote}[1]{\normalmarginpar \marginpar{\tiny #1}}
\usepackage[margin=0.8in]{geometry}

\setenumerate[0]{label=(\alph*)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\noindent {\large {\bf HUGH HAN, XIAOCHEN LI, SHIJIE WU} \\ \\ {\bf 600.465 Natural Language Processing} \hfill {{\bf Fall 2016}}}\\
{{\bf Homework \#1}} \hfill {{\bf Due:} 21 September 2016, 2pm} \\ \\
\rule[0.1in]{\textwidth}{0.4pt}
	
\section{Initial Implementation}
	The following output was produced using \code{./randsent grammar 10}.
	
	\begin{lstlisting}
is it true that every sandwich under every floor kissed the sandwich ?
is it true that the pickle under the pickle on a delicious pickled president pickled the president under the pickle ?
the president on the president in every fine floor under the sandwich in every president under a chief of staff ate the floor under a chief of staff on the pickle on a floor in every chief of staff with the perplexed president on the president .
a chief of staff pickled the sandwich in a floor on the sandwich with the president with the fine floor in every sandwich in a floor on a pickle on every sandwich with every sandwich under the pickle under a sandwich on the president under a pickled sandwich in the delicious floor on the pickled pickled pickle under the chief of staff in every president on the perplexed pickle on the pickle in every president on every floor on a sandwich in the chief of staff in every pickle in the pickle with a floor on the sandwich with every floor on a president with a chief of staff with a floor on every floor in every chief of staff in a sandwich on every floor on the president on the president under the chief of staff in every pickle under every chief of staff with every pickle with every pickle on every president on the pickle .
a pickle on the sandwich with every sandwich on the chief of staff under the chief of staff on a chief of staff under every pickle with every president in a pickle under every pickle on a chief of staff with the pickle under every pickle on a chief of staff under every delicious sandwich under a pickle with every president under the president in the sandwich in every pickle in the sandwich under the pickled chief of staff with every sandwich under the floor with a delicious floor with a pickle in a pickled chief of staff under every floor under every sandwich in every floor under every chief of staff under every chief of staff in every sandwich with the chief of staff with the president under the pickle under every pickle with every pickle in the floor in every president with the president with a pickled sandwich on a president under a floor in the fine floor in a chief of staff under a chief of staff under every floor in a perplexed fine floor with a president with the president under a president in the delicious delicious floor on every pickle on the pickle under a fine pickle with the chief of staff with every chief of staff on the sandwich in every sandwich on every chief of staff in the sandwich under a pickle in every delicious perplexed sandwich with the floor with a perplexed chief of staff with the president on the floor on the pickle in a pickle under every floor ate the chief of staff !
every delicious president wanted the fine pickle .
is it true that the pickle wanted the sandwich ?
a floor wanted a president !
every pickle with every chief of staff kissed a delicious fine president with a president .
a chief of staff with a floor pickled the perplexed perplexed fine sandwich .
	\end{lstlisting}
(Note: the indentation, or ``tabs'', seen in this sample output were not present in the generation of these sentences. They are printed here to help distinguish each sentence from the other. The same applies for all of the sample outputs throughout this \LaTeX \ document.)
\newpage

\section{Initial Discussion}

\begin{enumerate}
\item 
	The rule 
	\begin{center} \code{NP} $\to$ \code{NP PP} \end{center}
	is responsible for all these long sentences. 
	
To explain why, let's first name the nonterminals that are not preterminals "true-nonterminals" because they do not turn into terminals in one step. Mostly, a preterminal (except ``Noun'') will eventually turn into one word in the final sentence, while a true-nonterminal will always turn into more than one word. Generally, true-nonterminals have more potential to generate long sentences.

Looking at all the rules that turn true-nonterminals into other symbols, we can find two rules turn a true-nonterminal to more than one true-nonterminals. they are 
\begin{enumerate}[label=(\roman*)]
\item \code{S} $\to$ \code{NP VP},
\item \code{NP} $\to$ \code{NP PP}.
\end{enumerate}
However, rule (i) is the only rule that turns \code{S} into other symbols---there are no other possible options  \code{S} is given. (And with our grammar, we are \textit{always} given \code{S} right after \code{ROOT}.) 

When given \code{NP}, it's a bit different. We have the possibility of creating terminals (i.e. turning \code{NP} into \code{Det Noun}. However, if we use rule (ii) when \code{NP} is given, then \code{NP} becomes \code{NP PP}. Because \code{PP} can only turn into \code{Prep NP}, we are then left with an expression containing \textit{two} \code{NP}s: \code{NP Prep NP}. 

Rule (ii) creates something sort of like an explosion: one \code{NP} could give us two, and two \code{NP}s could give us even more. And this is why we generate so many long sentences!

\item 
	The intuition here is that we have six rules--all of equal probability--that turn the symbol \code{Noun} into other symbol(s), with only one of those rules generating an \code{Adj}. 
	(That rule is \code{Noun} $\to$ \code{Adj Noun}). \vspace{4pt} \\
Now let's do some calculations. Suppose there are $n$ \code{Noun}s. Because the probability of any rule being applied is uniformly random, we can choose a rule for \code{Noun} uniformly randomly. \vspace{4pt} \\
After one ``step'' of the rule is applied to all $n$ \code{Noun}s in the sentence, we will have an expected $\frac{5n}{6}$ words and $\frac{n}{6}$ \code{Adj Noun}s. After the second step, we have an expected $\frac{5n}{6^2}$ \code{Adj Noun}s and $\frac{n}{6^2}$ \code{Adj Adj Noun}s. \vspace{4pt} \\
In fact, we can generalize the expected number of consecutive \code{Adj}s:
\begin{eqnarray*}
	\expct{\code{Adj}} &=& \frac{n}{6^2} + \frac{n}{6^3} + \frac{n}{6^4} + \cdots \\
		&=& \sum\limits_{i=2}^\infty \frac{n}{6^i} \\
		&=& \left( \sum\limits_{i=1}^\infty \frac{n}{6^i} \right) - \frac{n}{6} \\
		&=& \frac{n}{5} - \frac{n}{6} \\
		&=& \frac{n}{30} \\
\end{eqnarray*}
That is, the expected number of consecutive \code{Adj}s appearing next to each other is only one-thirtieth of the number of \code{Noun}s that appear in the sentence.
\item 
	This has been implemented in \code{Grammar.py}.
\item
	To reduce the number of long sentences, we have to reduce the possibility that a nonterminal changes into another nonterminal. To increase the number of adjectives, we can simply increase the probability that a \code{Noun} changes into a \code{Adj Noun}. 
	
	In our grammar, we changed the rules:
	\begin{enumerate} 
	\item $\text{\code{1 NP}} \to \text{\code{NP PP}}$ into $\text{\code{0.2 NP}} \to \text{\code{NP PP}}$,
	\item $\text{\code{1 Noun}} \to \text{\code{Adj Noun}}$ into $\text{\code{3 Noun}} \to \text{\code{Adj Noun}}$.
	\end{enumerate}
	Change (i) solves the problem of long sentences, in that it reduces the likelihood of a single nonterminal turning into multiple nonterminals.
	
	Change (ii) solves the problem of too few adjectives, in that it increases the likelihood of adjective generation.
\item
	The changes made for \code{grammar2} are:
	\begin{center} \begin{tabular}{l l l}
	\code{2}    & \code{ROOT} & \code{S .} \\
	\code{\#}    & \code {...} & \ \\
	\code{0.3} & \code{Verb} & \code{pickled} \\
	\code{0.5} & \code{Det} & \code{every} \\
	\code{0.5} & \code{Noun} & \code{pickle} \\
	\code{0.5} & \code{Noun} & \code{chief of staff} \\
	\code{1.5} & \code{Adj} & \code{fine} \\
	\code{1.5} & \code{Prep} & \code{on} \\
	\code{1.5} & \code{Prep} & \code{in} 
	\end{tabular} \end{center}
	
	The reasoning for these changes are:
        \begin{itemize}
        \item[-] a sentence ending with a period is more common than either a sentence ending with an exclamation point or a sentence ending with a question mark,
        \item[-] the verb ``pickled'' is not frequently used in the English language,
        \item[-] the frequencies are picked relative to the frequencies of other words of the same type.
        \end{itemize}
        
        Ten sentences generated with the new grammar are pasted below:
        \begin{lstlisting}
the chief of staff wanted a pickle .
the president pickled a president .
the floor wanted every president on the sandwich on the sandwich !
is it true that the president wanted every pickle ?
a delicious pickled pickle wanted the sandwich !
a chief of staff ate the sandwich .
the pickled floor under the president with the pickled floor wanted the fine fine floor !
a perplexed president under the fine pickled perplexed delicious sandwich ate the floor with the pickle with the floor .
a chief of staff understood a floor !
a floor kissed a perplexed perplexed sandwich .
        \end{lstlisting}
	
\end{enumerate}

\newpage

\section{Modifications}

\begin{lstlisting}
is it true that the floor kissed Sally and thought that the floor wanted Xiaochen and Xiaochen ?
Sally understood a pickle on Xiaochen or Xiaochen .
a president and the really fine pickled pickle kissed the delicious floor on Sally with Xiaochen with Xiaochen and the sandwich and the sandwich with Sally on Sally under the very pickled chief of staff and the pickled sandwich or Xiaochen .
is it true that every fine pickled delicious delicious floor pickled the delicious president with Sally ?
it understood Xiaochen and a pickle or Xiaochen under a floor or the floor that the delicious chief of staff kissed a pickled floor !
is it true that the president in Sally thought that Xiaochen pickled a pickled president under Sally and the sandwich ?
is it true that Xiaochen flew with Xiaochen and Sally under the perplexed sandwich or Sally on a sandwich on every chief of staff on the delicious pickled perplexed pickled floor on a pickle and the very fine pickle with Sally and the delicious sandwich ?
a floor and Xiaochen thought that that Xiaochen worked on and ate the fine pickled fine floor worked on the really fine fine president under Sally !
is it true that a pickled fine perplexed sandwich thought that that it understood and ate Xiaochen that that it pickled Sally or the sandwich and every president that the president kissed Sally under Sally understood Sally kissed Sally under Xiaochen or Sally in Xiaochen ?
the very really very very very pickled floor wanted the delicious floor .
it understood the floor that the pickle flew and kissed a perplexed sandwich !
\end{lstlisting}
(This problem is continued on the next page.)

\newpage

\hspace{-22pt} There were a couple of things that we had to add to be able to generate the phenomena illustrated in each of the parts \textbf{3}(a) to \textbf{3}(h).
\begin{enumerate}
\item 
	``Sally'' is a name, which is a proper noun. The thing special about is that a proper noun does not have any determiner. A proper noun, or \code{Noun\_pro}, was defined in \code{grammar3}, into which \code{NP} could turn. 
\item 
	Two transitive verbs must be able to be next to each other, separated only by a conjunction. A rule was implemented to allow \code{Verb} to turn into \code{Verb Conj Verb}. Note that a \code{Verb} is strictly a transitive verb. 
	
	Two verb phrases must also be able to be next to each other, separated by a conjunction like in the following sentences:
	
	\code{Sally ate an apple and ate an apple .} \\
	\code{Sally flew and flew .}
	
	To allow this, a rule was implemented to allow \code{VP} to turn into \code{VP Conj VP}.
	
	Finally, two noun phrases must be able to appear next to each other, separated by a conjunction. To allow this, a rule was implemented to allow \code{NP} to turn into \code{NP Conj NP}.
\item 
	An intransitive verb, or \code{V\_intran}, was defined in \code{grammar3}. A rule was implemented to allow \code{VP} to turn into \code{V\_intran}.
\item 
	The word ``thought'' is a special verb that can be followed by \code{that S}. In fact, there are a whole class of verbs that can be followed by \code{that S}, including the words ``think'', ``understand'', ``believe'', ``decide'', etc. This class of verbs, or \code{V\_that} was defined in \code{grammar3}. \code{thought} was then defined as something that \code{V\_that} could turn into. A rule was implemented such that \code{VP} could turn into \code{V\_that N\_clause}, where \code{N\_clause} is a noun clause, which then turns into \code{that S}.
\item 
	We can think about the sentence as ``it'' is doing something to a noun phrase, such that something else happens. A new rule was implemented such that \code{S} could turn into \code{it Verb NP N\_clause}.
\item 
	We can think about the sentence as something happening, which in effect does something to a noun phrase. A new rule was implemented such that \code{S} could turn into \code{N\_clause Verb NP}.
\item 
	``very'' is an adverb, which can be used to describe an adjective. First, an adverb, or \code{Adv}, was defined in \code{grammar3}, into which an adjective, or \code{Adj}, could turn. Next, \code{very} was defined as a word that \code{Adv} could turn into.
\item
	First, the word \code{worked} was introduced to \code{grammar3} as an intransitive verb (\code{V\_intran}). Next, a new rule was implemented such that \code{VP} could turn into \code{VP PP}. This allows any type of action--both verbs and intransitive verbs--to take place \textit{on}, (or \textit{with}, \textit{under}, etc.) something.
	
	However, note that the phrase ``worked on'' is actually quite special in itself. In the sentence
	
	\code{the president worked on every proposal on the desk .}
	
	``worked'' could be applied as an intransitive verb, but that would apply that the president was physically on top of every proposal on the desk, \textit{while} he was working. 
	
	Instead, the more logical meaning of the sentence if a \textit{human} were to read it would be that the president was working on--or in the process of completing--every proposal that was currently on the desk. In this case, ``worked'' is actually \textit{not} an intransitive verb; rather, the phrase ``worked on'' acts as a verb itself. Because of this reasoning, we also added the phrase \code{worked on} as a transitive verb, or \code{Verb}, to \code{grammar3}.
\end{enumerate} 

\newpage

\section{Trees, Pretty Prints, Brackets}
\begin{lstlisting}
is it true that {[Xiaochen] ate [Xiaochen]} ? 
(ROOT is 
       it 
       true 
       that 
       (S (NP (Name Xiaochen)) 
           (VP (Verb ate) 
                (NP (Name Xiaochen)))) 
       ?) 
       
is it true that {[a very delicious chief of staff] understood [a perplexed floor]} ? 
(ROOT is 
       it 
       true 
       that 
       (S (NP (Det a) 
               (Noun (Adj (Adv very) 
                           (Adj delicious)) 
                      (Noun chief 
                             of 
                             staff))) 
          (VP (Verb understood) 
               (NP (Det a) 
                    (Noun (Adj perplexed) 
                           (Noun floor))))) 
       ?) 
       
is it true that {[Xiaochen] wanted [a fine sandwich]} ? 
(ROOT is 
      it 
      true 
      that 
      (S (NP (Name Xiaochen)) 
          (VP (Verb wanted) 
               (NP (Det a) 
                    (Noun (Adj fine) 
                           (Noun sandwich))))) 
      ?) 
      
{it kissed [Xiaochen] that {[Sally] understood [Sally]}} . 
(ROOT (S it 
          (Verb kissed) 
          (NP (Name Xiaochen)) 
          (N_clause that 
                    (S (NP (Name Sally)) 
                       (VP (Verb understood) 
                            (NP (Name Sally)))))) 
      .) 
      
{[Xiaochen] flew} ! 
(ROOT (S (NP (Name Xiaochen)) 
          (VP (V_intran flew))) 
      !) 
\end{lstlisting}

\section{Alternate Derivations}
\begin{enumerate}
\item 
	The first (given) derivation is:
\begin{lstlisting}
(ROOT (S (NP (NP (NP (Det every) 
                       (Noun sandwich)) 
                   (PP (Prep with) 
                       (NP (Det a) 
                           (Noun pickle)))) 
               (PP (Prep on) 
                    (NP (Det the) 
                         (Noun floor)))) 
           (VP (Verb wanted) 
                (NP (Det a) 
                     (Noun president))))
      .)
\end{lstlisting}
      
      An alternate derivation is as follows:
\begin{lstlisting}
(ROOT (S (NP (NP (Det every)
                   (Noun sandwich)) 
               (PP (Prep with) 
                   (NP (NP (Det a) 
                           (Noun pickle)))
                       (PP (Prep on) 
                           (NP (Det the) 
                               (Noun floor)))))
           (VP (Verb wanted) 
               (NP (Det a) 
                   (Noun president))))
      .)
\end{lstlisting}
\item
	In the first derivation, the sandwich was on the floor, but the pickle was not. 
	
	In the second derivation, the pickle was on the floor, but the sandwich was not. 
	
	These two sentences are both grammatically correct, but have different literal meanings in English. Therefore, it is necessary to note these differences.
\end{enumerate}

\newpage

\section{Parse}
\begin{enumerate}
\item
	The parser does not always recover the original derivation that was ``intended'' by \code{randsent}. In fact, we can give an example of a sentence for which \code{parse} ``misunderstood'' and found an alternative derivation. \vspace{4pt} \\
	Our sentence was generated via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that 
                   (S (N_clause that 
                                (S (NP (NP (NP (NP (Noun_pro Sally))                      
                                               (Conj and)                                 
                                               (NP (Det a) 
                                                   (Noun (Adj (Adv really)                
                                                              (Adj fine))                 
                                                         (Noun pickle))))                 
                                           (Conj and) 
                                           (NP (NP (Det the) 
                                                   (Noun sandwich))                       
                                               (PP (Prep in) 
                                                   (NP (NP (NP (Noun_pro Xiaochen))       
                                                           (PP (Prep in) 
                                                               (NP (Noun_pro Sally))))    
                                                       (PP (Prep with) 
                                                           (NP (Det the) 
                                                               (Noun sandwich)))))))      
                                       (PP (Prep on) 
                                           (NP (Det every) 
                                               (Noun pickle))))                           
                                   (VP (Verb wanted) 
                                       (NP (Noun_pro Sally)))))                           
                      (VP (VP (Verb kissed)                                               
                              (NP (Det a) 
                                  (Noun (Adj (Adv really)                                 
                                             (Adj fine)) 
                                        (Noun sandwich))))                                
                          (Conj and) 
                          (VP (Verb wanted) 
                              (NP (Noun_pro Xiaochen))))))                                
         (VP (V_intran flew)))                                                            
      .)
      
	\end{lstlisting}
	(This problem is continued on the next page.)
	\newpage
	
	However, \code{parse} found an alternate derivation via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that
                   (S (N_clause that
                                (S (NP (NP (NP (NP (NP (NP (Noun_pro Sally))
                                                       (Conj and)
                                                       (NP (NP (Det a)
                                                               (Noun (Adj (Adv really)
                                                                          (Adj fine))
                                                                     (Noun pickle)))
                                                           (Conj and)
                                                           (NP (Det the)
                                                               (Noun sandwich))))
                                                   (PP (Prep in)
                                                       (NP (Noun_pro Xiaochen))))
                                               (PP (Prep in)
                                                   (NP (Noun_pro Sally))))
                                           (PP (Prep with)
                                               (NP (Det the)
                                                   (Noun sandwich))))
                                       (PP (Prep on)
                                           (NP (Det every)
                                               (Noun pickle))))
                                   (VP (Verb wanted)
                                       (NP (Noun_pro Sally)))))
                      (VP (VP (Verb kissed)
                              (NP (Det a)
                                  (Noun (Adj (Adv really)
                                             (Adj fine))
                                        (Noun sandwich))))
                          (Conj and)
                          (VP (Verb wanted)
                              (NP (Noun_pro Xiaochen))))))
         (VP (V_intran flew)))
      .)
      
	\end{lstlisting}
	
	(Note that the two trees are not equivalent.) \vspace{4pt} \\ \\ \\
	The reason for this is that the ``correct'' tree of a specific sentence could be ambiguous, depending on the grammar that was used.  That is, if there is more than one unique tree that leads to the generated sentence, the parser could guess a tree that is different from what was actually used by the generator. \vspace{4pt} \\
In our example, the sentence was composed of several subtrees first split using the \code{NP Conj NP} rule, and then later on split using the \code{NP PP} rule. However, the parser guessed that the sentence was composed of subtrees that consecutively split using the \code{NP PP} rule, and did not use the \code{NP Conj NP} rule to split until much later on.
\newpage
\item
	There are 5 ways to analyze the noun phrase \vspace{4pt} \\
	\code{every sandwich with a pickle on the floor under the chief of staff} \vspace{4pt} \\
	using the original grammar \code{grammar}. \vspace{4pt} \\
	Let's first define the term ``modify'' as follows:
	
	Assume that the rule $\text{\code{NP}}_1 \to \text{\code{NP}}_2 \text{ \code{PP}}$ exists in our grammar. Then $\text{\code{PP}}$ modifies some $N \in \text{\code{Noun}}$ if $\text{\code{NP}}_2$ at some point generates $N$.
	
	In this sentence:
	\begin{enumerate}[label=(\roman*)]
	\item the \code{PP} ``with a pickle'' can only modify the \code{Noun} ``sandwich",
	\item the \code{PP} ``on the floor'' can modify the \code{Noun}s ``sandwich" or ``pickle'',
	\item the \code{PP} ``under the chief of staff'' can modify the \code{Noun}s ``sandwich", ``pickle'', or ``floor''.
	\end{enumerate}
	By this logic, there would seem to be $1 \cdot 2 \cdot 3 = 6$ ways to parse this sentence. \vspace{4pt} \\
	However, when ``on the floor'' modifies ``sandwich'', ``under the chief of staff'' cannot simultaneously modify ``sandwich''. \vspace{4pt} \\
	Thus, there remains 5 possible ways to parse the sentence.
\item
	Using \code{grammar}, longer sentences that were generated generally had a greater number of different parses---ranging up to integers larger than $2^{31}-1$ such that it caused a bit overflow. Shorter sentences had a smaller number---generally between 1 to 2 different parses. However, the number of different parses were either \textit{really} small or \textit{really} big, and rarely in between.
	
	Using \code{grammar3}, longer sentences occur much less frequently due to the fixes that were implemented to reduce repetitive sentences. Because of this, sentences were generally shorter and thus had a smaller number of different parses. However, even when the sentences were somewhat long, the number of different parses was often less than 100. 
	
	We suspect that long sentences have a large number of different parses when \code{grammar} is used, due to a lesser number of rules, which leads to ambiguity. When long sentences occur via \code{grammar3}, however, the sentences are more likely to be defined by more specific rules, and are therefore more restricted and less prone to ambiguity.
\item
	\begin{enumerate}[label=(\roman*)]
	\item
		Why is \code{p(best parse)} so small? Well, we can examine the probabilities used to generate this sentence, by first taking a look at the probabilities given from \code{grammar}.
		\begin{eqnarray*}
			\prob{\text{this sentence}} &=&  \prob{\code{ROOT}} \cdot \prob{\code{S . } | \code{ ROOT}} \cdot \prob{\code{NP VP } | \code{ S}} \cdot \prob{\code{Det Noun } | \code{ NP}} \cdot \\
			&& \prob{\code{the } | \code{ Det}} \cdot \prob{\code{president } | \code{ Noun}} \cdot \prob{\code{Verb NP } | \code{ VP}} \cdot \\
			&& \prob{\code{ate } | \code{ Verb}} \cdot \prob{\code{Det Noun } | \code{ NP}} \cdot \prob{\code{the } | \code{ Det}} \cdot \\
			&& \prob{\code{sandwich } | \code{ Noun}}
		\end{eqnarray*}
		
		\newpage
		
		Fair enough. Those are a lot of expressions multiplied together, which will probably produce a small number. But what does it equate, exactly?
		\begin{eqnarray*}
			\prob{\code{ROOT}}             &=& 1 \\
			\prob{\code{S .} | \code{ROOT}} &=& 1/3 \\
                        	\prob{\code{NP VP} | \code{S}}        &=& 1 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{president} | \code{Noun}} &=& 1/6 \\
                        	\prob{\code{Verb NP} | \code{VP}}     &=& 1 \\
                        	\prob{\code{ate} | \code{Verb}}      &=& 1/5 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{sandwich} | \code{Noun}}  &=& 1/6 \\
		\end{eqnarray*}
		$\implies \prob{\text{this sentence}} = 5.144032922 \times 10^{-5}$ \vspace{4pt} \\
		Let's call this number $p$, for simplicity purposes. \vspace{4pt} \\
		Using Bayes' Rule, we know that the probability that this is the best parse is equal to the probability that this sentence occurs, multiplied with the probability that we get the best parse \textit{conditioned} on the event of this sentence occurring. That is, 
		\begin{eqnarray*}
			\text{\code{p(best\_parse)}} &=& \text{\code{p(best\_parse|sentence)}} \cdot \text{\code{p(sentence)}}
		\end{eqnarray*}
		We know that \text{\code{p(best\_parse|sentence)}} $= 1$, and we just calculated $p$. Therefore,
		\begin{eqnarray*}
			\text{\code{p(best\_parse)}} &=& 1 \cdot \prob{\text{this sentence}} \\
			&=& 5.144032922 \times 10^{-5}
		\end{eqnarray*}
		Now let's think about why \code{p(sentence)} is equivalent to \code{p(best\_parse)}. 
		
		We can again use Bayes' Rule to mathematically argue this. But we can also think intuitively. \code{p(best\_parse)} is the probability that the given parse is the best possible parse. If we know that there is \textit{one} best parse for the specific generated sentence, it \textit{must} be that the probability that this parse is the best parse of \textit{all} best parses for \textit{all} possible sentences is equivalent to the probability of generating this sentence. \vspace{4pt} \\
		Finally, we can think about why \code{p(best\_parse|sentence)} $= 1$. 
		Using the same arguments as above, we know that there is only one possible ``best'' parse of the sentence. Therefore, the probability that the given parse is equivalent to the \textit{only} best parse is 100\%. \vspace{4pt} \\
		We can then check our reasoning: \vspace{4pt} \\
		\codebox{\$ ./parse -c -g grammar \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }} \\
		\codebox{the president ate the sandwich .} \vspace{4pt} \\
		and notice that there indeed is only 1 possible parse of the given sentence.
		
		\newpage
	\item
		Looking at the possibilities from \code{grammar}, we see that there is actually one other parse that could exist:
		\begin{lstlisting}
(ROOT (S (NP (NP (Det every)
                   (Noun sandwich))
               (PP (Prep with)
                   (NP (NP (Det a)
                            (Noun pickle))
                        (PP (Prep on)
                            (NP (Det the)
                                 (Noun floor))))))
           (VP (Verb wanted)
                (NP (Det a)
                     (Noun president))))
      .)
		\end{lstlisting}
		Now looking at the probabilities from \code{grammar} we see that the event that ``on the floor'' is applied to ``a pickle'' is equally as likely as the event that ``on the floor'' is applied to ``every sandwich with a pickle''.
		
		Because there are only two possible events, and their probabilities are equal, it must be that the probability of each one happening is exactly 0.5.
		\item
			First, we calculate the log-probability of the two sentences
			\begin{eqnarray*}
				- \text{log-probability} &=& - \log_2 p(s_1) - \log_2 p(s_2) \\
					&=& - \log_2 \left(5.144032922 \times 10^-5 \right) - \log_2 \left(1.240362877 \times 10^-9 \right) \\
                    &=& 43.8333311993
			\end{eqnarray*}
			where $s_1$ and $s_2$ are the events that the first sentence and second sentence occur, respectively.
			
            Next, we can divide the negative log-probability by 18, the number of words. The resulting cross-entropy is 2.435185067.
		\item 
			We can calculate perplexity by raising 2 to the power of the cross-entropy.
			\begin{eqnarray*}
				\text{perplexity} &=& 2^{2.435185067} \\
					&=& 5.4083370619
			\end{eqnarray*}
		\item
			The sentence ``\code{the sandwich ate .}'' is not a sentence that can occur using the grammar \code{grammar}. That is, 
			\begin{eqnarray*}
				\prob{\code{the sandwich ate .}} &=& 0
			\end{eqnarray*}
			Thus, taking the log-probability results in taking the negative logarithm of 0, which is undefined but approaches infinity.
	\end{enumerate}
	\item
		Calculating the entropy of \code{grammar2}: \vspace{4pt} \\
		\codebox{\$ ./randsent grammar2 1000 | ./parse -P -g grammar2 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ } \\
		\codebox{\# cross-entropy = 2.192609841 bits = -(-24335.77663 log-prob. / 11099 words)\ \ \ \ }
		
		Calculating the entropy of \code{grammar3}: \vspace{4pt} \\
		\codebox{\$ ./randsent grammar3 1000 | ./parse -P -g grammar3 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ } \\
		\codebox{\# cross-entropy = 2.730945129 bits = -(-58756.28445 log-prob. / 21515 words)\ \ \ \ }	
			
                The entropy of \code{grammar3} is larger than the entropy of \code{grammar2}.  This is because \code{grammar3} is able to generate a wider variety of sentences, which leads to a more ``creative'' corpus.
                
                Calculating the entropy of \code{grammar}: \vspace{4pt} \\
                \codebox{\$ ./randsent grammar3 1000 | ./parse -P -g grammar3 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ }
                
		The problem with calculating the entropy of \code{grammar} is that the rules in \code{grammar} allow for really \textit{really} long sentences, which take a very long time to generate. We can look at the possible rules for \code{NP} in \code{grammar}:
		\begin{eqnarray*}
			\text{\code{1}} &\text{\code{NP}}& \text{\code{Det Noun}} \\
			\text{\code{1}} &\text{\code{NP}}& \text{\code{NP PP}} \\
		\end{eqnarray*}
		It is clear that each rule has a 1/2 probability of being executed. If we calculate the probability that a sentence generated with \code{grammar} terminates, we actually see that it is exactly 1.
		\begin{eqnarray*}
			\prob{\text{sentence terminates}} &=& p \\
			p &=& \left( 1 - \frac{1}{2} \right) \cdot 1 + \frac{1}{2} \cdot p^2 \\
				&=& \frac{1}{2} + \frac{1}{2} \cdot p^2 \\
			\frac{1}{2}p^2 - p + \frac{1}{2} &=& 0 \\
			p &=& 1
		\end{eqnarray*}
		However, note that increasing the probability of \code{NP} $\to$ \code{NP PP} even ever so slightly would cause a probability $p$ such that $0 < p < 1$. That is, the probability of a generated sentence running on forever would non-zero. And because of this, \code{grammar} produces some really long sentences, which makes calculating the entropy a bit difficult (and slow).
	\item
		To compare the entropy of a corpus from \code{grammar2} to the grammar rules \code{grammar}, \code{grammar2}, and \code{grammar3}, it would be a good idea to keep the actual corpus constant. Thus, we can first generate a text file via \code{grammar2}, and then feed it into \code{parse} using \code{grammar}, \code{grammar2}, and \code{grammar3}.
		
		First, generating the corpus: \vspace{4pt} \\
		\codebox{\$ ./randsent grammar2 1000 > corpus2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  } \\
		
		Next, feeding in the corpus to each \code{parse}, using each grammar: \vspace{4pt} \\
		\codebox{\$ ./parse -P -g grammar < corpus2 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  } \\
		\codebox{\$ ./parse -P -g grammar2 < corpus2 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  } \\
		\codebox{\$ ./parse -P -g grammar3 < corpus2 | ./prettyprint \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  } \\
		
		Finally, the outputs, respectively: \vspace{4pt} \\
		\codebox{\# cross-entropy = 2.447528595 bits = -(-26467.57422 log-prob. / 10814 words)\ \ \ \ } \\
		\codebox{\# cross-entropy = 2.185994451 bits = -(-23639.344 log-prob. / 10814 words)\ \ \ \ \ \ } \\
		\codebox{\# cross-entropy = 2.598891774 bits = -(-28104.41565 log-prob. / 10814 words)\ \ \ \ } \\
				
		The prediction is correct; \code{grammar2} on average predicts this corpus better than \code{grammar} or \code{grammar3} does. 
		
		The reasoning for this could be that using the rules from \code{grammar} and \code{grammar3}, \code{parse} expects to see the sentences generated via \code{grammar2} less frequently than if using the rules from \code{grammar2} itself. 
\end{enumerate}
\section{Extensions}
The two parts we decided to table for this problem were \textbf{7}(a) and \textbf{7}(b).
\begin{enumerate}
\item
	We can use a new symbol \code{Det\_vo} to represent a determiner that can only be used before a word beginning with a vowel. In fact, there is only one word that \code{Det\_vo} turns into: \code{an}.
	
Note that in the English grammar, only a noun, adjective, or adverb can immediately follow a determiner.
Thus, we also need to define the symbols \code{Noun\_vo}, \code{Adj\_vo}, and \code{Adv\_vo} to represent nouns, adjectives, and adverbs, respectively, that begin with vowels.

To make the grammar file \code{grammar7} slightly easier to read, we can define \code{NOUN} to represent \textit{any} type of noun. That its, \code{NOUN} can turn into either \code{Noun} \textit{or} \code{Noun\_vo}.

Similarly, we can define \code{ADJ} and \code{ADV} to represent any type of adjective and adverb. 

Note that the new rules that have been added in the grammar for this question have comments along the lines of `` \code{\# this rule is for (7a)} ''.
\item
	To solve this problem, we can use the symbols in the below chart to represent verbs, sentences, verb phrases, and noun clauses in different tenses:
\begin{center} \begin{tabular}{l l l l}
    &                              \code{Past Tense} & \code{Present Tense} & \code{3rd-personal Singular Form}  \\
\code{Transitive Verb}   & \code{V\_tr\_ed} & \code{V\_tr\_ori}  & \code{V\_tr\_s} \\
\code{Intransitive Verb} & \code{V\_in\_ed} & \code{V\_in\_ori} & \code{V\_in\_s} \\
\code{Sentence}            & \code{S\_pas}     & \code{S\_pre}      &     \\
\code{Verb Phrase}       & \code{VP\_pas}   & \code{VP\_pre}   & \code{VPs\_pre} \\
\code{Noun Clause}      & \code{NC\_pas}   & \code{NC\_pre}  &                            \\
\end{tabular}\end{center}
	Each of grammar rules that was implemented before this step is in the past tense. These symbols were changed for less ambiguity.
	
	A yes-no question is composed of an auxiliary word, a sentence whose verb is in its original form, and a question mark at the end. So all we need to do to implement a yes-no question is add the following rule:
\begin{center}
	\code{ROOT} $\to$ \code{Aux S\_ori ?}
\end{center}
along with a series of rules to expand \code{S\_ori}.

Note that the new rules that have been added in the grammar for this question have comments along the lines of `` \code{\# this rule is for (7b)} ''.
\end{enumerate}
Sample output is provided below:
\begin{lstlisting}
can the extremely eloquent idea pickle a perplexed pickled floor and jump ?
could it be that that Xiaochen worked on the perplexed fine president ate the idea ?
Xiaochen kissed an apple .
the apple or the idea pickled a pickle .
that every idea ate an idea and wanted the pickle or wanted a sandwich worked on Sally in every extremely extremely pickled sandwich !
the president with the perplexed president kissed an eloquent sandwich with Xiaochen in Sally in a president .
can Sally work ?
could it be that Xiaochen sighed ?
the sandwich ate Xiaochen and kissed the pickle on a pickle !
Sally flew on a sandwich !
\end{lstlisting}

\section{English}
\begin{enumerate}
\item For the first part of \code{grammar8}, we would like to implement verbs in the infinitive form. That is, we want to be able to capture the structures of sentences like the following:
\begin{lstlisting}
Sally wanted to kiss every sandwich !               
Xiaochen failed to fly on every floor .             
the president managed to eat the floor .            
it is awesome to eat an apple and sigh .            
it is awesome for Sally to understand that a president sighed .
will it be possible for an apple to eat the floor ? 
\end{lstlisting}

There are several kinds of infinitives, and infinitives have the ability to generate ``infinitive phrase''s such as ``to fly on every floor'', ``to eat an apple'', ``to eat an apple and sigh'', and ``to understand that a president sighed''. We defined a new symbol \code{INFP} to represent these infinitive phrases:
\begin{eqnarray*}
	\code{INFP} &\to& \code{to VP\_pre}
\end{eqnarray*}

This rule can capture the structure in sentence (1)-(3):
\begin{eqnarray*}
	\code{VP\_pas} &\to& \code{V\_inf\_ed INFP}
\end{eqnarray*}
Then we need some more rules to make this rule working well with in grammar7:
\begin{eqnarray*}
\code{VP\_pre} &\to& \code{V\_inf\_ori INFP}\\
\code{V\_inf\_ed} &\to& \code{wanted}\\
\code{V\_inf\_ed} &\to& \code{managed}\\
\code{V\_inf\_ed} &\to& \code{failed}\\
\code{V\_inf\_ori} &\to& \code{want}\\
\code{V\_inf\_ori} &\to& \code{fail}\\
\code{V\_inf\_ori} &\to& \code{manage}
\end{eqnarray*}

We need to add this two rules to generate sentence (4):
\begin{eqnarray*}
\code{S} &\to& \code{it is ADJ INFP}\\
\code{Adj\_vo} &\to& \code{awesome}
\end{eqnarray*}

We use \code{ForToP} (For-To Phrase) to describe structures like "for Sally to understand that a president sighed" and "for an apple to eat the floor". These two rules are for (5):
\begin{eqnarray*}
\code{ForToP} &\to& \code{for NP INFP }\\
\code{S} &\to& \code{it is ADJ ForToP}
\end{eqnarray*}

This group of rules captures the structure in sentence (6):
\begin{eqnarray*}
\code{ROOT} &\to&  \code{Modal it be ADJ INFP ?}\\
\code{ROOT} &\to&  \code{Modal it be ADJ ForToP ?}\\
\code{Modal}  &\to& \code{can}\\
\code{Modal} &\to&  \code{could}\\
\code{Modal} &\to&  \code{will}\\
\code{Modal} &\to&  \code{should}\\
\code{Adj}   &\to&  \code{possible}
\end{eqnarray*}

\item For the second part of \code{grammar8}, we would like to implement the phenomena described in \textbf{7}(c) and \textbf{7}(d).

That is, relative clauses and WH-word questions were implemented into our grammar. Here are some examples of these phenomena:
\begin{lstlisting}
the pickle kissed the president that ate the sandwich .
the pickle kissed the sandwich that the president ate .
the pickle kissed the sandwich that the president thought that Sally ate .
what did the president think ?
what did the president think that Sally ate ?
what did Sally eat the sandwich with ?
who ate the sandwich ?
where did Sally eat the sandwich ?
\end{lstlisting}

These types of sentences all have sub-sentences without a direct object (or a
\code{Noun} after \code{Prep} in prepositional phrase), subject or
prepositional phrase. To represent these types of sub-sentences, we added the
following rules for sentences in the past tense.

\begin{eqnarray*}
\code{S\_ed\_wo\_obj} &\to&  \code{NP V\_that\_ed that S\_ed\_wo\_obj}\\
\code{S\_ed\_wo\_obj} &\to&  \code{NP V\_that\_ed that S\_ed\_wo\_sub}\\
\code{S\_ed\_wo\_obj} &\to&  \code{NP V\_tr\_ed}\\
\code{S\_ed\_wo\_obj} &\to&  \code{NP V\_tr\_ed PP}\\
\code{S\_ed\_wo\_obj} &\to&  \code{NP VP\_pas Prep}\\
\code{S\_ed\_wo\_obj} &\to&  \code{NP VP\_pas PP Prep}\\
\code{S\_ed\_wo\_sub} &\to&  \code{VP\_pas}\\
\end{eqnarray*}

Similarly, we could define \code{S\_ori\_wo\_obj} and \code{S\_ori\_wo\_sub} for sentences in the present tense.
\begin{eqnarray*}
\code{S\_ori\_wo\_obj} &\to&  \code{NP V\_that\_ori that S\_ori\_wo\_obj}\\
\code{S\_ori\_wo\_obj} &\to&  \code{NP V\_that\_ori that S\_ori\_wo\_sub}\\
\code{S\_ori\_wo\_obj} &\to&  \code{NP V\_tr\_ori}\\
\code{S\_ori\_wo\_obj} &\to&  \code{NP V\_tr\_ori PP}\\
\code{S\_ori\_wo\_obj} &\to&  \code{NP VP\_pre Prep}\\
\code{S\_ori\_wo\_obj} &\to&  \code{NP VP\_pre PP Prep}\\
\code{S\_ori\_wo\_sub} &\to&  \code{VP\_pre}\\
\end{eqnarray*}

And with the following rules, we can generate all of the above example
sentences. We also generalize the WH-word including "what", "who", "when",
"why", "how" and "where". We distinguish WH-word based on whether it can refer
to a Noun. For example, "what" and "who" can be used to refer a Noun while the
other WH-word cannot. We use \code{W\_word\_obj} to refer "what" and "who". For
the rest of WH-word, we use \code{W\_word}.
\begin{eqnarray*}
\code{ROOT} &\to&  \code{W\_word\_obj Aux S\_ori\_wo\_obj ?}\\
\code{ROOT} &\to&  \code{W\_word\_obj S\_ed\_wo\_sub ?}\\
\code{ROOT} &\to&  \code{W\_word Aux S\_ori ?}\\
\code{NP}   &\to&  \code{the Noun that S\_ed\_wo\_sub}\\
\code{NP}   &\to&  \code{the Noun that S\_ed\_wo\_obj}\\
\end{eqnarray*}

\end{enumerate}
\end{document}
