\documentclass[11pt]{article}

\usepackage{enumitem}               
 
\usepackage{titlesec}

\usepackage{listings}
\usepackage{color}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language={},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}

\newcommand{\N}{\ensuremath{\mathbb N}}

\newcommand{\prob}[1]{\ensuremath{\text{{\bf Pr}$\left[#1\right]$}}}
\newcommand{\expct}[1]{\ensuremath{\text{{\bf E}$\left[#1\right]$}}}

\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\codebox}[1]{\colorbox{codegray}{\texttt{#1}}}

\newcommand{\mnote}[1]{\normalmarginpar \marginpar{\tiny #1}}
\usepackage[margin=0.8in]{geometry}

\setenumerate[0]{label=(\alph*)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\noindent {\large {\bf HUGH HAN, XIAOCHEN LI, SHIJIE WU} \\ \\ {\bf 600.465 Natural Language Processing} \hfill {{\bf Fall 2016}}}\\
{{\bf Homework \#1}} \hfill {{\bf Due:} 21 September 2016, 2pm} \\ \\
\rule[0.1in]{\textwidth}{0.4pt}
	
\section{Initial Implementation}
	The following output was produced using \code{./randsent grammar 10}.
	
	\begin{lstlisting}
is it true that every sandwich under every floor kissed the sandwich ?
is it true that the pickle under the pickle on a delicious pickled president pickled the president under the pickle ?
the president on the president in every fine floor under the sandwich in every president under a chief of staff ate the floor under a chief of staff on the pickle on a floor in every chief of staff with the perplexed president on the president .
a chief of staff pickled the sandwich in a floor on the sandwich with the president with the fine floor in every sandwich in a floor on a pickle on every sandwich with every sandwich under the pickle under a sandwich on the president under a pickled sandwich in the delicious floor on the pickled pickled pickle under the chief of staff in every president on the perplexed pickle on the pickle in every president on every floor on a sandwich in the chief of staff in every pickle in the pickle with a floor on the sandwich with every floor on a president with a chief of staff with a floor on every floor in every chief of staff in a sandwich on every floor on the president on the president under the chief of staff in every pickle under every chief of staff with every pickle with every pickle on every president on the pickle .
a pickle on the sandwich with every sandwich on the chief of staff under the chief of staff on a chief of staff under every pickle with every president in a pickle under every pickle on a chief of staff with the pickle under every pickle on a chief of staff under every delicious sandwich under a pickle with every president under the president in the sandwich in every pickle in the sandwich under the pickled chief of staff with every sandwich under the floor with a delicious floor with a pickle in a pickled chief of staff under every floor under every sandwich in every floor under every chief of staff under every chief of staff in every sandwich with the chief of staff with the president under the pickle under every pickle with every pickle in the floor in every president with the president with a pickled sandwich on a president under a floor in the fine floor in a chief of staff under a chief of staff under every floor in a perplexed fine floor with a president with the president under a president in the delicious delicious floor on every pickle on the pickle under a fine pickle with the chief of staff with every chief of staff on the sandwich in every sandwich on every chief of staff in the sandwich under a pickle in every delicious perplexed sandwich with the floor with a perplexed chief of staff with the president on the floor on the pickle in a pickle under every floor ate the chief of staff !
every delicious president wanted the fine pickle .
is it true that the pickle wanted the sandwich ?
a floor wanted a president !
every pickle with every chief of staff kissed a delicious fine president with a president .
a chief of staff with a floor pickled the perplexed perplexed fine sandwich .
	\end{lstlisting}
(Note: the indentation, or ``tabs'', seen in this LaTeX document were not present in the generation of these sentences. They are there to help distinguish each sentence from the other.)
\newpage

\section{Initial Discussion}

\begin{enumerate}
\item 
	The rule 
	\begin{center} \code{NP} $\to$ \code{NP PP} \end{center}
	is responsible for all these long sentences. 
	
To explain why, let's first name the nonterminals that are not preterminals "true-nonterminals" because they do not turn into terminals in one step. Mostly, a preterminal (except ``Noun'') will eventually turn into one word in the final sentence, while a true-nonterminal will always turn into more than one word. Generally, true-nonterminals have more potential to generate long sentences.

Looking at all the rules that turn true-nonterminals into other symbols, we can find two rules turn a true-nonterminal to more than one true-nonterminals. they are 
\begin{enumerate}[label=(\roman*)]
\item \code{S} $\to$ \code{NP VP},
\item \code{NP} $\to$ \code{NP PP}.
\end{enumerate}
However, rule (i) is the only rule that turns \code{S} into other symbols---there are no other possible options  \code{S} is given. (And with our grammar, we are \textit{always} given \code{S} right after \code{ROOT}.) 

When given \code{NP}, it's a bit different. We have the possibility of creating terminals (i.e. turning \code{NP} into \code{Det Noun}. However, if we use rule (ii) when \code{NP} is given, then \code{NP} becomes \code{NP PP}. Because \code{PP} can only turn into \code{Prep NP}, we are then left with an expression containing \textit{two} \code{NP}s: \code{NP Prep NP}. 

Rule (ii) creates something sort of like an explosion: one \code{NP} could give us two, and two \code{NP}s could give us even more. And this is why we generate so many long sentences!

\item 
	The intuition here is that we have six rules--all of equal probability--that turn the symbol \code{Noun} into other symbol(s), with only one of those rules generating an \code{Adj}. 
	(That rule is \code{Noun} $\to$ \code{Adj Noun}). \vspace{4pt} \\
Now let's do some calculations. Suppose there are $n$ \code{Noun}s. Because the probability of any rule being applied is uniformly random, we can choose a rule for \code{Noun} uniformly randomly. \vspace{4pt} \\
After one ``step'' of the rule is applied to all $n$ \code{Noun}s in the sentence, we will have an expected $\frac{5n}{6}$ words and $\frac{n}{6}$ \code{Adj Noun}s. After the second step, we have an expected $\frac{5n}{6^2}$ \code{Adj Noun}s and $\frac{n}{6^2}$ \code{Adj Adj Noun}s. \vspace{4pt} \\
In fact, we can generalize the expected number of consecutive \code{Adj}s:
\begin{eqnarray*}
	\expct{\code{Adj}} &=& \frac{n}{6^2} + \frac{n}{6^3} + \frac{n}{6^4} + \cdots \\
		&=& \sum\limits_{i=2}^\infty \frac{n}{6^i} \\
		&=& \left( \sum\limits_{i=1}^\infty \frac{n}{6^i} \right) - \frac{n}{6} \\
		&=& \frac{n}{5} - \frac{n}{6} \\
		&=& \frac{n}{30} \\
\end{eqnarray*}
That is, the expected number of consecutive \code{Adj}s appearing next to each other is only one-thirtieth of the number of \code{Noun}s that appear in the sentence.
\item 
	This has been implemented in \code{Grammar.py}.
\item
	To reduce the number of long sentences, we have to reduce the possibility that a nonterminal changes into another nonterminal. To increase the number of adjectives, we can simply increase the probability that a \code{Noun} changes into a \code{Adj Noun}. 
	
	In our grammar, we changed the rules:
	\begin{enumerate} 
	\item $\text{\code{1 NP}} \to \text{\code{NP PP}}$ into $\text{\code{0.2 NP}} \to \text{\code{NP PP}}$,
	\item $\text{\code{1 Noun}} \to \text{\code{Adj Noun}}$ into $\text{\code{3 Noun}} \to \text{\code{Adj Noun}}$.
	\end{enumerate}
	Change (i) solves the problem of long sentences, in that it reduces the likelihood of a single nonterminal turning into multiple nonterminals.
	
	Change (ii) solves the problem of too few adjectives, in that it increases the likelihood of adjective generation.
\item
	The changes made for \code{grammar2} are:
	\begin{center} \begin{tabular}{l l l}
	\code{2}    & \code{ROOT} & \code{S .} \\
	\code{\#}    & \code {...} & \ \\
	\code{0.3} & \code{Verb} & \code{pickled} \\
	\code{0.5} & \code{Det} & \code{every} \\
	\code{0.5} & \code{Noun} & \code{pickle} \\
	\code{0.5} & \code{Noun} & \code{chief of staff} \\
	\code{1.5} & \code{Adj} & \code{fine} \\
	\code{1.5} & \code{Prep} & \code{on} \\
	\code{1.5} & \code{Prep} & \code{in} 
	\end{tabular} \end{center}
	
	The reasoning for these changes are:
        \begin{itemize}
        \item[-] a sentence ending with a period is more common than either a sentence ending with an exclamation point or a sentence ending with a question mark,
        \item[-] the verb ``picked'' is not frequently used in the English language,
        \item[-] the frequencies are picked relative to the frequencies of other words of the same type.
        \end{itemize}
        
        Ten sentences generated with the new grammar are pasted below:
        \begin{lstlisting}
the chief of staff wanted a pickle .
the president pickled a president .
the floor wanted every president on the sandwich on the sandwich !
is it true that the president wanted every pickle ?
a delicious pickled pickle wanted the sandwich !
a chief of staff ate the sandwich .
the pickled floor under the president with the pickled floor wanted the fine fine floor !
a perplexed president under the fine pickled perplexed delicious sandwich ate the floor with the pickle with the floor .
a chief of staff understood a floor !
a floor kissed a perplexed perplexed sandwich .
        \end{lstlisting}
        (Note: the indentation, or ``tabs'', seen in this LaTeX document were not present in the generation of these sentences. They are there to help distinguish each sentence from the other.)
	
\end{enumerate}

\section{Modifications}

\newpage

\section{Trees, Pretty Prints, Brackets}
\begin{lstlisting}
is it true that {[Xiaochen] ate [Xiaochen]} ? 
(ROOT is 
       it 
       true 
       that 
       (S (NP (Name Xiaochen)) 
           (VP (Verb ate) 
                (NP (Name Xiaochen)))) 
       ?) 
       
is it true that {[a very delicious chief of staff] understood [a perplexed floor]} ? 
(ROOT is 
       it 
       true 
       that 
       (S (NP (Det a) 
               (Noun (Adj (Adv very) 
                           (Adj delicious)) 
                      (Noun chief 
                             of 
                             staff))) 
          (VP (Verb understood) 
               (NP (Det a) 
                    (Noun (Adj perplexed) 
                           (Noun floor))))) 
       ?) 
       
is it true that {[Xiaochen] wanted [a fine sandwich]} ? 
(ROOT is 
      it 
      true 
      that 
      (S (NP (Name Xiaochen)) 
          (VP (Verb wanted) 
               (NP (Det a) 
                    (Noun (Adj fine) 
                           (Noun sandwich))))) 
      ?) 
      
{it kissed [Xiaochen] that {[Sally] understood [Sally]}} . 
(ROOT (S it 
          (Verb kissed) 
          (NP (Name Xiaochen)) 
          (N_clause that 
                    (S (NP (Name Sally)) 
                       (VP (Verb understood) 
                            (NP (Name Sally)))))) 
      .) 
      
{[Xiaochen] flew} ! 
(ROOT (S (NP (Name Xiaochen)) 
          (VP (V_intran flew))) 
      !) 
\end{lstlisting}

\section{Alternate Derivations}
\begin{enumerate}
\item 
	The first (given) derivation is:
\begin{lstlisting}
(ROOT (S (NP (NP (NP (Det every) 
                       (Noun sandwich)) 
                   (PP (Prep with) 
                       (NP (Det a) 
                           (Noun pickle)))) 
               (PP (Prep on) 
                    (NP (Det the) 
                         (Noun floor)))) 
           (VP (Verb wanted) 
                (NP (Det a) 
                     (Noun president))))
      .)
\end{lstlisting}
      
      An alternate derivation is as follows:
\begin{lstlisting}
(ROOT (S (NP (NP (Det every)
                   (Noun sandwich)) 
               (PP (Prep with) 
                   (NP (NP (Det a) 
                           (Noun pickle)))
                       (PP (Prep on) 
                           (NP (Det the) 
                               (Noun floor)))))
           (VP (Verb wanted) 
               (NP (Det a) 
                   (Noun president))))
      .)
\end{lstlisting}
\item
	In the first derivation, the sandwich was on the floor, but the pickle was not. 
	
	In the second derivation, the pickle was on the floor, but the sandwich was not. 
	
	These two sentences are both grammatically correct, but have different literal meanings in English. Therefore, it is necessary to note these differences.
\end{enumerate}

\section{Parse}
\begin{enumerate}
\item
	The parser does not always recover the original derivation that was ``intended'' by \code{randsent}. In fact, we can give an example of a sentence for which \code{parse} ``misunderstood'' and found an alternative derivation. \vspace{4pt} \\
	Our sentence was generated via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that 
                   (S (N_clause that 
                                (S (NP (NP (NP (NP (Noun_pro Sally))                      
                                               (Conj and)                                 
                                               (NP (Det a) 
                                                   (Noun (Adj (Adv really)                
                                                              (Adj fine))                 
                                                         (Noun pickle))))                 
                                           (Conj and) 
                                           (NP (NP (Det the) 
                                                   (Noun sandwich))                       
                                               (PP (Prep in) 
                                                   (NP (NP (NP (Noun_pro Xiaochen))       
                                                           (PP (Prep in) 
                                                               (NP (Noun_pro Sally))))    
                                                       (PP (Prep with) 
                                                           (NP (Det the) 
                                                               (Noun sandwich)))))))      
                                       (PP (Prep on) 
                                           (NP (Det every) 
                                               (Noun pickle))))                           
                                   (VP (Verb wanted) 
                                       (NP (Noun_pro Sally)))))                           
                      (VP (VP (Verb kissed)                                               
                              (NP (Det a) 
                                  (Noun (Adj (Adv really)                                 
                                             (Adj fine)) 
                                        (Noun sandwich))))                                
                          (Conj and) 
                          (VP (Verb wanted) 
                              (NP (Noun_pro Xiaochen))))))                                
         (VP (V_intran flew)))                                                            
      .)
      
	\end{lstlisting}
	(This problem is continued on the next page.)
	\newpage
	
	However, \code{parse} found an alternate derivation via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that
                   (S (N_clause that
                                (S (NP (NP (NP (NP (NP (NP (Noun_pro Sally))
                                                       (Conj and)
                                                       (NP (NP (Det a)
                                                               (Noun (Adj (Adv really)
                                                                          (Adj fine))
                                                                     (Noun pickle)))
                                                           (Conj and)
                                                           (NP (Det the)
                                                               (Noun sandwich))))
                                                   (PP (Prep in)
                                                       (NP (Noun_pro Xiaochen))))
                                               (PP (Prep in)
                                                   (NP (Noun_pro Sally))))
                                           (PP (Prep with)
                                               (NP (Det the)
                                                   (Noun sandwich))))
                                       (PP (Prep on)
                                           (NP (Det every)
                                               (Noun pickle))))
                                   (VP (Verb wanted)
                                       (NP (Noun_pro Sally)))))
                      (VP (VP (Verb kissed)
                              (NP (Det a)
                                  (Noun (Adj (Adv really)
                                             (Adj fine))
                                        (Noun sandwich))))
                          (Conj and)
                          (VP (Verb wanted)
                              (NP (Noun_pro Xiaochen))))))
         (VP (V_intran flew)))
      .)
      
	\end{lstlisting}
	
	(Note that the two trees are not equivalent.) \vspace{4pt} \\ \\ \\
	The reason for this is that the ``correct'' tree of a specific sentence could be ambiguous, depending on the grammar that was used.  That is, if there is more than one unique tree that leads to the generated sentence, the parser could guess a tree that is different from what was actually used by the generator. \vspace{4pt} \\
In our example, the sentence was composed of several subtrees first split using the \code{NP Conj NP} rule, and then later on split using the \code{NP PP} rule. However, the parser guessed that the sentence was composed of subtrees that consecutively split using the \code{NP PP} rule, and did not use the \code{NP Conj NP} rule to split until much later on.
\newpage
\item
	There are 5 ways to analyze the noun phrase \vspace{4pt} \\
	\code{every sandwich with a pickle on the floor under the chief of staff} \vspace{4pt} \\
	using the original grammar \code{grammar}. \vspace{4pt} \\
	Let's first define the term ``modify'' as follows:
	
	Assume that the rule $\text{\code{NP}}_1 \to \text{\code{NP}}_2 \text{ \code{PP}}$ exists in our grammar. Then $\text{\code{PP}}$ modifies some $N \in \text{\code{Noun}}$ if $\text{\code{NP}}_2$ at some point generates $N$.
	
	In this sentence:
	\begin{enumerate}[label=(\roman*)]
	\item the \code{PP} ``with a pickle'' can only modify the \code{Noun} ``sandwich",
	\item the \code{PP} ``on the floor'' can modify the \code{Noun}s ``sandwich" or ``pickle'',
	\item the \code{PP} ``under the chief of staff'' can modify the \code{Noun}s ``sandwich", ``pickle'', or ``floor''.
	\end{enumerate}
	By this logic, there would seem to be $1 \cdot 2 \cdot 3 = 6$ ways to parse this sentence. \vspace{4pt} \\
	However, when ``on the floor'' modifies ``sandwich'', ``under the chief of staff'' cannot simultaneously modify ``sandwich''. \vspace{4pt} \\
	Thus, there remains 5 possible ways to parse the sentence.
\item
\item
	\begin{enumerate}[label=(\roman*)]
	\item
		Why is \code{p(best parse)} so small? Well, we can examine the probabilities used to generate this sentence, by first taking a look at the probabilities given from \code{grammar}.
		\begin{eqnarray*}
			\prob{\text{this sentence}} &=&  \prob{\code{ROOT}} \cdot \prob{\code{S .} | \code{ROOT}} \cdot \prob{\code{NP VP} | \code{S}} \cdot \prob{\code{Det Noun} | \code{NP}} \cdot \\
			&& \prob{\code{the} | \code{Det}} \cdot \prob{\code{president} | \code{Noun}} \cdot \prob{\code{Verb NP} | \code{VP}} \cdot \\
			&& \prob{\code{ate} | \code{Verb}} \cdot \prob{\code{Det Noun} | \code{NP}} \cdot \prob{\code{the} | \code{Det}} \cdot \prob{\code{sandwich} | \code{Noun}}
		\end{eqnarray*}
		Fair enough. Those are a lot of expressions multiplied together, which will probably produce a small number. But what does it equate, exactly?
		\begin{eqnarray*}
			\prob{\code{ROOT}}             &=& 1 \\
			\prob{\code{S .} | \code{ROOT}} &=& 1/3 \\
                        	\prob{\code{NP VP} | \code{S}}        &=& 1 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{president} | \code{Noun}} &=& 1/6 \\
                        	\prob{\code{Verb NP} | \code{VP}}     &=& 1 \\
                        	\prob{\code{ate} | \code{Verb}}      &=& 1/5 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{sandwich} | \code{Noun}}  &=& 1/6 \\
		\end{eqnarray*}
		$\implies \prob{\text{this sentence}} = 5.144032922 \times 10^{-5}$ \vspace{4pt} \\
		Let's call this number $p$, for simplicity purposes. \vspace{4pt} \\
		Using Bayes' Rule, we know that the probability that this is the best parse is equal to the probability that this sentence occurs, multiplied with the probability that we get the best parse \textit{conditioned} on the event of this sentence occurring. That is, 
		\begin{eqnarray*}
			\text{\code{p(best\_parse)}} &=& \text{\code{p(best\_parse|sentence)}} \cdot \text{\code{p(sentence)}}
		\end{eqnarray*}
		We know that \text{\code{p(best\_parse|sentence)}} $= 1$, and we just calculated $p$. Therefore,
		\begin{eqnarray*}
			\text{\code{p(best\_parse)}} &=& 1 \cdot \prob{\text{this sentence}} \\
			&=& 5.144032922 \times 10^{-5}
		\end{eqnarray*}
		Now let's think about why \code{p(sentence)} is equivalent to \code{p(best\_parse)}. 
		
		We can again use Bayes' Rule to mathematically argue this. But we can also think intuitively. \code{p(best\_parse)} is the probability that the given parse is the best possible parse. If we know that there is \textit{one} best parse for the specific generated sentence, it \textit{must} be that the probability that this parse is the best parse of \textit{all} best parses for \textit{all} possible sentences is equivalent to the probability of generating this sentence. \vspace{4pt} \\
		Finally, we can think about why \code{p(best\_parse|sentence)} $= 1$. 
		Using the same arguments as above, we know that there is only one possible ``best'' parse of the sentence. Therefore, the probability that the given parse is equivalent to the \textit{only} best parse is 100\%. \vspace{4pt} \\
		We can then check our reasoning: \vspace{4pt} \\
		\codebox{\$ ./parse -c -g grammar \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }} \\
		\codebox{the president ate the sandwich .} \vspace{4pt} \\
		and notice that there indeed is only 1 possible parse of the given sentence.
	\item
		Looking at the possibilities from \code{grammar}, we see that there is actually one other parse that could exist:
		\begin{lstlisting}
(ROOT (S (NP (NP (Det every)
                   (Noun sandwich))
               (PP (Prep with)
                   (NP (NP (Det a)
                            (Noun pickle))
                        (PP (Prep on)
                            (NP (Det the)
                                 (Noun floor))))))
           (VP (Verb wanted)
                (NP (Det a)
                     (Noun president))))
      .)
		\end{lstlisting}
		Now looking at the probabilities from \code{grammar} we see that the event that ``on the floor'' is applied to ``a pickle'' is equally as likely as the event that ``on the floor'' is applied to ``every sandwich with a pickle''.
		
		Because there are only two possible events, and their probabilities are equal, it must be that the probability of each one happening is exactly 0.5.
		\item
		\item
		\item
	\end{enumerate}
\end{enumerate}
\section{}
\begin{enumerate}
\item
\item
\item
\item
\item
\item
\item
\end{enumerate}

\end{document}