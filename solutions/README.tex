\documentclass[11pt]{article}

\usepackage{enumitem}               
 
\usepackage{titlesec}

\usepackage{listings}
\usepackage{color}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language={},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}

\newcommand{\N}{\ensuremath{\mathbb N}}

\newcommand{\prob}[1]{\ensuremath{\text{{\bf Pr}$\left[#1\right]$}}}
\newcommand{\expct}[1]{\ensuremath{\text{{\bf E}$\left[#1\right]$}}}

\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\codebox}[1]{\colorbox{codegray}{\texttt{#1}}}

\newcommand{\mnote}[1]{\normalmarginpar \marginpar{\tiny #1}}
\usepackage[margin=0.8in]{geometry}

\setenumerate[0]{label=(\alph*)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\noindent {\large {\bf HUGH HAN, XIAOCHEN LI, SHIJIE WU} \\ \\ {\bf 600.465 Natural Language Processing} \hfill {{\bf Fall 2016}}}\\
{{\bf Homework \#1}} \hfill {{\bf Due:} 21 September 2016, 2pm} \\ \\
\rule[0.1in]{\textwidth}{0.4pt}
	
\section{Random Sentence Generator}
	The following output was produced using \code{./randsent grammar 10}.
	
	\begin{lstlisting}
is it true that every sandwich under every floor kissed the sandwich ?
is it true that the pickle under the pickle on a delicious pickled president pickled the president under the pickle ?
the president on the president in every fine floor under the sandwich in every president under a chief of staff ate the floor under a chief of staff on the pickle on a floor in every chief of staff with the perplexed president on the president .
a chief of staff pickled the sandwich in a floor on the sandwich with the president with the fine floor in every sandwich in a floor on a pickle on every sandwich with every sandwich under the pickle under a sandwich on the president under a pickled sandwich in the delicious floor on the pickled pickled pickle under the chief of staff in every president on the perplexed pickle on the pickle in every president on every floor on a sandwich in the chief of staff in every pickle in the pickle with a floor on the sandwich with every floor on a president with a chief of staff with a floor on every floor in every chief of staff in a sandwich on every floor on the president on the president under the chief of staff in every pickle under every chief of staff with every pickle with every pickle on every president on the pickle .
a pickle on the sandwich with every sandwich on the chief of staff under the chief of staff on a chief of staff under every pickle with every president in a pickle under every pickle on a chief of staff with the pickle under every pickle on a chief of staff under every delicious sandwich under a pickle with every president under the president in the sandwich in every pickle in the sandwich under the pickled chief of staff with every sandwich under the floor with a delicious floor with a pickle in a pickled chief of staff under every floor under every sandwich in every floor under every chief of staff under every chief of staff in every sandwich with the chief of staff with the president under the pickle under every pickle with every pickle in the floor in every president with the president with a pickled sandwich on a president under a floor in the fine floor in a chief of staff under a chief of staff under every floor in a perplexed fine floor with a president with the president under a president in the delicious delicious floor on every pickle on the pickle under a fine pickle with the chief of staff with every chief of staff on the sandwich in every sandwich on every chief of staff in the sandwich under a pickle in every delicious perplexed sandwich with the floor with a perplexed chief of staff with the president on the floor on the pickle in a pickle under every floor ate the chief of staff !
every delicious president wanted the fine pickle .
is it true that the pickle wanted the sandwich ?
a floor wanted a president !
every pickle with every chief of staff kissed a delicious fine president with a president .
a chief of staff with a floor pickled the perplexed perplexed fine sandwich .
	\end{lstlisting}
	
\newpage

\section{Questions}

\begin{enumerate}
\item 
	The rule 
	\begin{center} \code{NP} $\to$ \code{NP PP} \end{center}
	is responsible for all these long sentences. 
	
To explain why, let's first name the nonterminals that are not preterminals "true-nonterminals" because they do not turn into terminals in one step. Mostly, one preterminal (except ``Noun'') will eventually turn into one word in the final sentence, while a true-nonterminal always turns into more than one words. So generally, true-nonterminals have more potential to generate long sentences.

Looking at all the rules that turn true-nonterminals into other symbols, we can find that only two rules turn a true-nonterminal to more than one true-nonterminals. they are 
\begin{enumerate}[label=(\roman*)]
\item \code{S} $\to$ \code{NP VP},
\item \code{NP} $\to$ \code{NP PP}.
\end{enumerate}
However, rule (i) is the only rule that turns \code{S} into other symbols---there is no other choice when \code{S} in the sentence. Things are different when it comes rule (ii). We actually can turn \code{NP} into \code{Det Noun} by another rule; and if we use rule (ii), \code{NP} becomes \code{NP PP}. Because \code{PP} can only turn into \code{Prep NP}, we have two \code{NP}s now: \code{NP Prep NP}. So it's like an explosion: one \code{NP} gives us two, and two \code{NP}s give us more. All thanks to the rule \code{NP} $\to$ \code{NP PP}.
\item 
\item 
\item
\item
\end{enumerate}

\section{}
\begin{enumerate}
\item
\end{enumerate}

\section{}
\begin{enumerate}
\item
\end{enumerate}

\section{}
\begin{enumerate}
\item
\end{enumerate}

\section{Parse}
\begin{enumerate}
\item
\end{enumerate}

\section{}
\begin{enumerate}
\item
	The parser does not always recover the original derivation that was ``intended'' by \code{randsent}. In fact, we can give an example of a sentence for which \code{parse} ``misunderstood'' and found an alternative derivation. \vspace{4pt} \\
	Our sentence was generated via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that 
                   (S (N_clause that 
                                (S (NP (NP (NP (NP (Noun_pro Sally))                      
                                               (Conj and)                                 
                                               (NP (Det a) 
                                                   (Noun (Adj (Adv really)                
                                                              (Adj fine))                 
                                                         (Noun pickle))))                 
                                           (Conj and) 
                                           (NP (NP (Det the) 
                                                   (Noun sandwich))                       
                                               (PP (Prep in) 
                                                   (NP (NP (NP (Noun_pro Xiaochen))       
                                                           (PP (Prep in) 
                                                               (NP (Noun_pro Sally))))    
                                                       (PP (Prep with) 
                                                           (NP (Det the) 
                                                               (Noun sandwich)))))))      
                                       (PP (Prep on) 
                                           (NP (Det every) 
                                               (Noun pickle))))                           
                                   (VP (Verb wanted) 
                                       (NP (Noun_pro Sally)))))                           
                      (VP (VP (Verb kissed)                                               
                              (NP (Det a) 
                                  (Noun (Adj (Adv really)                                 
                                             (Adj fine)) 
                                        (Noun sandwich))))                                
                          (Conj and) 
                          (VP (Verb wanted) 
                              (NP (Noun_pro Xiaochen))))))                                
         (VP (V_intran flew)))                                                            
      .)
      
	\end{lstlisting}
	(This problem is continued on the next page.)
	\newpage
	
	However, \code{parse} found an alternate derivation via the following tree:
	\begin{lstlisting}
	
(ROOT (S (N_clause that
                   (S (N_clause that
                                (S (NP (NP (NP (NP (NP (NP (Noun_pro Sally))
                                                       (Conj and)
                                                       (NP (NP (Det a)
                                                               (Noun (Adj (Adv really)
                                                                          (Adj fine))
                                                                     (Noun pickle)))
                                                           (Conj and)
                                                           (NP (Det the)
                                                               (Noun sandwich))))
                                                   (PP (Prep in)
                                                       (NP (Noun_pro Xiaochen))))
                                               (PP (Prep in)
                                                   (NP (Noun_pro Sally))))
                                           (PP (Prep with)
                                               (NP (Det the)
                                                   (Noun sandwich))))
                                       (PP (Prep on)
                                           (NP (Det every)
                                               (Noun pickle))))
                                   (VP (Verb wanted)
                                       (NP (Noun_pro Sally)))))
                      (VP (VP (Verb kissed)
                              (NP (Det a)
                                  (Noun (Adj (Adv really)
                                             (Adj fine))
                                        (Noun sandwich))))
                          (Conj and)
                          (VP (Verb wanted)
                              (NP (Noun_pro Xiaochen))))))
         (VP (V_intran flew)))
      .)
      
	\end{lstlisting}
	
	(Note that the two trees are not equivalent.) \vspace{4pt} \\ \\ \\
	The reason for this is that the ``correct'' tree of a specific sentence could be ambiguous, depending on the grammar that was used.  That is, if there is more than one unique tree that leads to the generated sentence, the parser could guess a tree that is different from what was actually used by the generator. \vspace{4pt} \\
In our example, the sentence was composed of several subtrees first split using the \code{NP Conj NP} rule, and then later on split using the \code{NP PP} rule. However, the parser guessed that the sentence was composed of subtrees that consecutively split using the \code{NP PP} rule, and did not use the \code{NP Conj NP} rule to split until much later on.
\newpage
\item
\item
\item
	\begin{enumerate}[label=(\roman*)]
	\item
		Why is \code{p(best parse)} so small? Well, we can examine the probabilities used to generate this sentence, by first taking a look at the probabilities given from \code{grammar}.
		\begin{eqnarray*}
			\prob{\text{this sentence}} &=&  \prob{\code{ROOT}} \cdot \prob{\code{S .} | \code{ROOT}} \cdot \prob{\code{NP VP} | \code{S}} \cdot \prob{\code{Det Noun} | \code{NP}} \cdot \\
			&& \prob{\code{the} | \code{Det}} \cdot \prob{\code{president} | \code{Noun}} \cdot \prob{\code{Verb NP} | \code{VP}} \cdot \\
			&& \prob{\code{ate} | \code{Verb}} \cdot \prob{\code{Det Noun} | \code{NP}} \cdot \prob{\code{the} | \code{Det}} \cdot \prob{\code{sandwich} | \code{Noun}}
		\end{eqnarray*}
		Fair enough. Those are a lot of expressions multiplied together, which will probably produce a small number. But what does it equate, exactly?
		\begin{eqnarray*}
			\prob{\code{ROOT}}             &=& 1 \\
			\prob{\code{S .} | \code{ROOT}} &=& 1/3 \\
                        	\prob{\code{NP VP} | \code{S}}        &=& 1 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{president} | \code{Noun}} &=& 1/6 \\
                        	\prob{\code{Verb NP} | \code{VP}}     &=& 1 \\
                        	\prob{\code{ate} | \code{Verb}}      &=& 1/5 \\
                        	\prob{\code{Det Noun} | \code{NP}}    &=& 1/2 \\
                        	\prob{\code{the} | \code{Det}}        &=& 1/3 \\
                        	\prob{\code{sandwich} | \code{Noun}}  &=& 1/6 \\
		\end{eqnarray*}
		$\implies \prob{\text{this sentence}} = 5.144032922 \times 10^{-5}$ \vspace{4pt} \\
		Let's call this number $p$, for simplicity purposes.
		Now let's think about why \code{p(sentence)} is equivalent to \code{p(best\_parse)}. The probability of getting this specific sentence was $p$. Given that we have some $k \in \N$ number of possible parses of all of the possible sentences generated from \code{grammar}, the probability of getting the correct ``best'' parse of a sentence should \textit{also} be $p$. That is, there should probably only be one possible parsing of each sentence, although some ``types'' of sentences could be more likely to appear than others. \vspace{4pt} \\
		Finally, we can think about why \code{p(best\_parse|sentence)} $= 1$. If we run: \vspace{4pt} \\
		\codebox{\$ ./parse -c -g grammar \text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }} \\
		\codebox{the president ate the sandwich .} \vspace{4pt} \\
		we see that the number of possible parses of the sentence is 1. Therefore, there is only one possible ``best'' parse of the sentence, so the probability that the given parse is equivalent to the \textit{only} best parse is 100\%.
	\item
	\end{enumerate}
\item
\end{enumerate}

\end{document}